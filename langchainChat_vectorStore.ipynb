{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import utils_zh\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "# langchain是一种快速开发应用程序框架，组件可以链式组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "     # 故意添加重复文档，使数据混乱\n",
    "    PyPDFLoader(\"data/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"data/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"data/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"data/cs229_lectures/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wq/yyg3qh714970yjrhkxr_q3640000gn/T/ipykernel_46075/1549816252.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833843782523142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#点积，分数越高句子越相似\n",
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21894574322018356"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18502072045263457"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorStores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma是因为它轻量级且数据存储在内存中\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory='chroma/'\n",
    "# !rm -rf 'chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall hnswlib\n",
    "# pip uninstall chroma-hnswlib\n",
    "# pip install chroma-hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,#pdf\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory#缓存路径\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is there an email i can ask for help\"  # \"有我可以寻求帮助的电子邮件吗\"\n",
    "docs = vectordb.similarity_search(question, k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we  usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.  \\nAs far as this class, I've also written, a nd I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the te chnical content of this  class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on  that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \\nnewsgroup that's sort of a forum for people in  the class to get to  know each other and \\nhave whatever discussions you want to ha ve amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to ta lk trash about this class there.  \\nIf you want to contact the teaching staff, pl ease use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 失败的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 8, 'source': 'data/cs229_lectures/MachineLearning-Lecture01.pdf'}, page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs = vectordb.similarity_search(question, k = 5)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'data/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 14, 'source': 'data/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 6, 'source': 'data/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': 'data/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 2, 'source': 'data/cs229_lectures/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about regression in the third lecture?\"  # \"他们在第三讲中是怎么谈论回归的？\"\n",
    "docs = vectordb.similarity_search(question, k=5)\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression problem like this. What I want to do today is talk about a class of algorithms \n",
      "called non-parametric learning algorithms that will help to alleviate the need somewhat \n",
      "for you to choose features very carefully. Okay ? And this leads us in to our discussion of \n",
      "locally weighted regression. And just to de fine the term, linear regression, as we’ve \n",
      "defined it so far, is an example of a parame tric learning algorithm. Parametric learning \n",
      "algorithm is one that’s defined as an algorithm that has a fixed number of parameters that \n",
      "fit to the data. Okay? So in linear regression we  have a fix set of parameters theta, right? \n",
      "That must fit to the data. In contrast, what  I’m gonna talk about now is our first non-\n",
      "parametric learning algorithm. The formal defi nition, which is not very  intuitive, so I’ve \n",
      "replaced it with a second, say, more intuitive. The, sort of, formal definition of the non-\n",
      "parametric learning algorithm is that it’s an  algorithm where the number of parameters \n",
      "goes with M, with the size of the training se t. And usually it’s de fined as a number of \n",
      "parameters grows linearly with the size of the training set. Th is is the formal definition. A \n",
      "slightly less formal definition is that th e amount of stuff that your learning algorithm \n",
      "needs to keep around will grow linearly with th e training sets or, in another way of saying \n",
      "it, is that this is an algorithm that we’ll n eed to keep around an entire training set, even\n"
     ]
    }
   ],
   "source": [
    "print(docs[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
